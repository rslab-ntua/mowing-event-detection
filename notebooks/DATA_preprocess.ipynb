{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ad705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###find number of mowings in each shapefile###\n",
    "import fiona\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "area = 6\n",
    "sum_of_areas = 0\n",
    "sum_of_events = 0\n",
    "for i in range(1,area+1):\n",
    "    shapefile_path = f'/mowing_annotation_{i}.shp'\n",
    "    # List for count\n",
    "    count_values = []\n",
    "    with fiona.open(shapefile_path, \"r\") as shapefile:\n",
    "        count_values = [feature[\"properties\"][\"count\"] for feature in shapefile]\n",
    "    unique_numbers, counts = np.unique(count_values, return_counts=True)\n",
    "    print(f'Sum of mowned areas for area {i}',sum(counts))\n",
    "    print(f'Sum of mowing events for area {i}',sum(count_values))\n",
    "    sum_of_areas += sum(counts)\n",
    "    sum_of_events += sum(count_values)\n",
    "    plt.bar(unique_numbers, counts, color='blue', alpha=0.7)\n",
    "    plt.xlabel('Number of Events')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.title(f'Mowing Event per Class in area {i}')\n",
    "    plt.yticks(np.arange(min(counts), max(counts)+1, 4))\n",
    "    plt.show()\n",
    "    \n",
    "print('Sum of mowned areas',sum_of_areas)\n",
    "print('Sum of mowing events',sum_of_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea4022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_path = f'/mowing_annotation_{17}.shp'\n",
    "    # List for count\n",
    "count_values = []\n",
    "with fiona.open(shapefile_path, \"r\") as shapefile:\n",
    "    count_values = [feature[\"properties\"][\"count\"] for feature in shapefile]\n",
    "unique_numbers, counts = np.unique(count_values, return_counts=True)\n",
    "print(f'Sum of mowned areas for area {i}',sum(counts))\n",
    "print(f'Sum of mowing events for area {i}',sum(count_values))\n",
    "sum_of_areas += sum(counts)\n",
    "sum_of_events += sum(count_values)\n",
    "plt.bar(unique_numbers, counts, color='blue', alpha=0.7)\n",
    "plt.xlabel('Number of Events')\n",
    "plt.ylabel('Counts')\n",
    "plt.title(f'Mowing Event per Class in area {i}')\n",
    "plt.yticks(np.arange(min(counts), max(counts)+1, 4))\n",
    "plt.show()\n",
    "    \n",
    "print('Sum of mowned areas',sum_of_areas)\n",
    "print('Sum of mowing events',sum_of_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b16286",
   "metadata": {},
   "outputs": [],
   "source": [
    "###iterate through a dict and values and save each polygon to a folder associatedd with its class###\n",
    "import fiona\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "input_folder = '/data/'\n",
    "export_folder_path = '/parcel_level_data/'\n",
    "\n",
    "INDEX_DESCRTPTION = 'SAR'\n",
    "area_dict = {'1':[0,1,2,3,4],'2':[0,1,2,3,4],\n",
    "            '3':[0,1,2,3,4],'4':[0,1,2,3,4],\n",
    "            '5':[0,1,2,3,4],'6':[0,1,2,3,4]}\n",
    "\n",
    "\n",
    "for i,key in enumerate(area_dict):\n",
    "    print('area is',key)\n",
    "    values = area_dict[key]\n",
    "    print('values are',values)\n",
    "    for j,value in enumerate(values):\n",
    "        area = key\n",
    "        target_attribute_value = value\n",
    "        shapefile_path = f'/mowing_annotation_{area}.shp'\n",
    "\n",
    "        with fiona.open(shapefile_path, \"r\") as shapefile:\n",
    "\n",
    "            selected_shapes = [feature[\"geometry\"] for feature in shapefile if feature[\"properties\"][\"count\"] == target_attribute_value]\n",
    "\n",
    "        for k,shape in enumerate(selected_shapes):\n",
    "\n",
    "            with rasterio.open(input_folder+f'kampos_collective_{area}/kampos_{area}_image_export{INDEX_DESCRTPTION}.tif') as src:\n",
    "                # Mask the raster based on the selected shapefile geometries\n",
    "                print('entered rasterio')\n",
    "                out_image, out_transform = rasterio.mask.mask(src, [shape], crop=True, nodata=np.nan)\n",
    "                out_meta = src.meta  # Get metadata from the source raster\n",
    "                \n",
    "            # Update metadata for the output raster\n",
    "            out_meta.update({\"driver\": \"GTiff\",\n",
    "                             \"height\": out_image.shape[1],\n",
    "                             \"width\": out_image.shape[2],\n",
    "                             \"transform\": out_transform})\n",
    "            #break\n",
    "            print(export_folder_path+str(target_attribute_value)+'/'+str(i)+str(j)+str(k)+'.tif')\n",
    "            output_raster_path = export_folder_path+str(target_attribute_value)+'/'+str(i)+str(j)+str(k)+'.tif'\n",
    "            with rasterio.open(output_raster_path, \"w\", **out_meta) as dest:\n",
    "                dest.write(out_image)\n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a52d8a-a641-44a2-b042-aa7a85c3a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "###iterate through a dict and values and save each polygon to a folder associatedd with its class for pretrain data###\n",
    "import fiona\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "###change folder, where the ts is saved###\n",
    "input_folder = '/area_4_preproc/'\n",
    "###change folder, where the data must be saved saved###\n",
    "export_folder_path = '/pretrain_area_4/'\n",
    "###change the index according to the file name###\n",
    "INDEX_DESCRTPTION = 'ts_area_4'\n",
    "\n",
    "area_dict = {'17':[0]}\n",
    "#area_dict = {'17':[0,1,2,3,4]}\n",
    "\n",
    "for i,key in enumerate(area_dict):\n",
    "    print('area is',key)\n",
    "    values = area_dict[key]\n",
    "    print('values are',values)\n",
    "    for j,value in enumerate(values):\n",
    "        area = key\n",
    "        target_attribute_value = value\n",
    "        ###enter the right shapefile name###\n",
    "        shapefile_path = f'/2021_ger_sieved_{area}_clipped_30_shape.shp'\n",
    "\n",
    "        with fiona.open(shapefile_path, \"r\") as shapefile:\n",
    "            selected_shapes = [feature[\"geometry\"] for feature in shapefile if feature[\"properties\"][\"count\"] == target_attribute_value]\n",
    "            #print(selected_shapes)\n",
    "       \n",
    "        for k,shape in enumerate(tqdm(selected_shapes)):\n",
    "            #print(shape.keys(),'area =',area,'target_attribute_value =',target_attribute_value)\n",
    "            #print(type(shape))\n",
    "            #print(shape) \n",
    "            #shape = json.dumps(shape, ensure_ascii=False)\n",
    "            #print(list[shape]) \n",
    "            \n",
    "            #print(export_folder_path+str(target_attribute_value)+'/'+str(i)+str(j)+str(k))\n",
    "            ###enter the right timeseries tif file name###\n",
    "            with rasterio.open(input_folder+f'thunen_{area}_{INDEX_DESCRTPTION}.tif') as src:\n",
    "                # Mask the raster based on the selected shapefile geometries\n",
    "                #print('entered rasterio')\n",
    "                out_image, out_transform = rasterio.mask.mask(src, [shape], crop=True, nodata=np.nan)\n",
    "                out_meta = src.meta  # Get metadata from the source raster\n",
    "                \n",
    "            \n",
    "            out_meta.update({\"driver\": \"GTiff\",\n",
    "                             \"height\": out_image.shape[1],\n",
    "                             \"width\": out_image.shape[2],\n",
    "                             \"transform\": out_transform})\n",
    "            #break\n",
    "            #print(export_folder_path+str(target_attribute_value)+'/'+str(i)+str(j)+str(k)+'.tif')\n",
    "            output_raster_path = export_folder_path+str(target_attribute_value)+'/'+str(i)+str(j)+str(k)+'.tif'\n",
    "            with rasterio.open(output_raster_path, \"w\", **out_meta) as dest:\n",
    "                dest.write(out_image)\n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12aa32a-d889-4554-9e2e-fef738c80875",
   "metadata": {},
   "outputs": [],
   "source": [
    "###iterate through a dict and values and save each polygon to a folder associatedd with its class for HLS data###\n",
    "import fiona\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# input folder for the timeseries data to crop\n",
    "input_folder = '/timeseries_areas/'\n",
    "\n",
    "\n",
    "## export folder for the dataset##\n",
    "##each different class--> different folder##\n",
    "export_folder_path = '/hls_data_mix/'\n",
    "INDEX_DESCRTPTION = '_full_hls'\n",
    "#'WNDVI'\n",
    "area_dict = {'17':[0,1,2,3,4]}\n",
    "\n",
    "\n",
    "for i,key in enumerate(area_dict):\n",
    "    print('area is',key)\n",
    "    values = area_dict[key]\n",
    "    print('values are',values)\n",
    "    for j,value in enumerate(values):\n",
    "        area = key\n",
    "        target_attribute_value = value\n",
    "        shapefile_path = f'/mowing_annotation_hls_{area}.shp'\n",
    "\n",
    "        with fiona.open(shapefile_path, \"r\") as shapefile:\n",
    "            selected_shapes = [feature[\"geometry\"] for feature in shapefile if feature[\"properties\"][\"count\"] == target_attribute_value]\n",
    "            \n",
    "       \n",
    "        for k,shape in enumerate(selected_shapes):\n",
    "\n",
    "            with rasterio.open(input_folder+f'kampos_{area}_image_export{INDEX_DESCRTPTION}.tif') as src:\n",
    "\n",
    "                out_image, out_transform = rasterio.mask.mask(src, [shape], crop=True, nodata=np.nan)\n",
    "                out_meta = src.meta  \n",
    "                \n",
    "            \n",
    "            out_meta.update({\"driver\": \"GTiff\",\n",
    "                             \"height\": out_image.shape[1],\n",
    "                             \"width\": out_image.shape[2],\n",
    "                             \"transform\": out_transform})\n",
    "\n",
    "            output_raster_path = export_folder_path+str(target_attribute_value)+'/'+str(i)+str(j)+str(k)+'.tif'\n",
    "            with rasterio.open(output_raster_path, \"w\", **out_meta) as dest:\n",
    "                dest.write(out_image)\n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e976ca-205e-4aee-bac8-690ef46fcf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd553f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###json of labels as in previous###\n",
    "###create a list of dicts with available data###\n",
    "import os\n",
    "import json\n",
    "\n",
    "def create_class_filename_json(data_path, output_file='../gt_jsons/myfile_area_4.json'):\n",
    "    class_filename_list = []\n",
    "    for class_folder in os.listdir(data_path):\n",
    "        class_path = os.path.join(data_path, class_folder)\n",
    "\n",
    "        if os.path.isdir(class_path):\n",
    "            for filename in os.listdir(class_path):\n",
    "                if filename.endswith('.tif'):\n",
    "                    class_filename_list.append({os.path.splitext(filename)[0]: class_folder})\n",
    "    #print(class_filename_list)\n",
    "    with open(output_file, \"w\") as out_file:\n",
    "        json.dump(class_filename_list, out_file)\n",
    "    \n",
    "\n",
    "data_path = '/pretrain_area_4/'\n",
    "result_dict = create_class_filename_json(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0038c8-bb8f-46f4-8a0a-bf5fe7b19292",
   "metadata": {},
   "outputs": [],
   "source": [
    "###here we do the split for each area based on the area_X###\n",
    "import json\n",
    "\n",
    "for i in range(1,7):\n",
    "    \n",
    "    file_path = 'channels.json'\n",
    "    with open(file_path, 'r') as file:\n",
    "        original_json = json.load(file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('the len of original json is',len(original_json))\n",
    "    \n",
    "    \n",
    "    desired_area = f'area_{i}'\n",
    "    \n",
    "\n",
    "    filtered_data = [{key: value} for item in original_json for key, value in item.items() if key.startswith(desired_area)]\n",
    "    print('the len of the filtered data is', len(filtered_data))\n",
    "    \n",
    "    output_file_path = f'area_split_{desired_area}_val.json'\n",
    "    print('saving filtered in',output_file_path)\n",
    "    \n",
    "    #print(filtered_data)\n",
    "    \n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        json.dump(filtered_data, output_file, indent=2)\n",
    "    \n",
    "    print(f\"Filtered data for {desired_area} saved to {output_file_path}\")\n",
    "    \n",
    "    print('entering train split section')\n",
    "    \n",
    "    filtered_train = []\n",
    "    for entry in original_json:\n",
    "        entry_keys = entry.keys()\n",
    "        if any(desired_area not in key for key in entry_keys):\n",
    "            print(f\"NOT Found {desired_area} in keys for entry:\", entry)\n",
    "            filtered_train.append(entry)\n",
    "        \n",
    "\n",
    "    \n",
    "    output_file_path_train = f'area_split_{desired_area}_train.json'\n",
    "    \n",
    "    print('saving train split to path', output_file_path_train)\n",
    "\n",
    "    with open(output_file_path_train, 'w') as output_file:\n",
    "        json.dump(filtered_train, output_file, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977e3fdb-1263-4c3a-b105-53b7a3e08786",
   "metadata": {},
   "outputs": [],
   "source": [
    "###create the train json that does not contain the validation split###\n",
    "file_path = 'myfile_area_split_all.json'\n",
    "desired_area = 'area_5'\n",
    "\n",
    "\n",
    "# Read JSON data from the file\n",
    "with open(file_path, 'r') as file:\n",
    "    original_json = json.load(file)\n",
    "\n",
    "filtered = []\n",
    "for entry in original_json:\n",
    "    entry_keys = entry.keys()\n",
    "    if any(desired_area not in key for key in entry_keys):\n",
    "        print(f\"Found {desired_area} in keys for entry:\", entry)\n",
    "        filtered.append(entry)\n",
    "    \n",
    "\n",
    "print(filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38cb121",
   "metadata": {},
   "outputs": [],
   "source": [
    "###split the files###\n",
    "import json\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "with open('../gt_jsons/myfile_thunen_area_4.json', 'r') as file:\n",
    "    json_data = file.read()\n",
    "    \n",
    "data_dict = json.loads(json_data)\n",
    "result_list = shuffle(data_dict)\n",
    "train_ratio = 0.8  \n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1  \n",
    "\n",
    "total_size = len(result_list)\n",
    "train_size = int(total_size * train_ratio)\n",
    "val_size = int(total_size * val_ratio)\n",
    "\n",
    "train_data = result_list[:train_size]\n",
    "val_data = result_list[train_size:train_size + val_size]\n",
    "test_data = result_list[train_size + val_size:]\n",
    "\n",
    "\n",
    "train_data = train_data\n",
    "val_data = val_data\n",
    "test_data = test_data\n",
    "\n",
    "print('length of train is',len(train_data))\n",
    "print('length of validation is',len(val_data))\n",
    "print('length of test is',len(test_data))\n",
    "\n",
    "\n",
    "\n",
    "with open('../gt_jsons/train_thunen_area_4.json', 'w') as file:\n",
    "    json.dump(train_data, file, ensure_ascii=False)\n",
    "\n",
    "with open('../gt_jsons/val_thunen_area_4.json', 'w') as file:\n",
    "    json.dump(val_data, file, ensure_ascii=False)\n",
    "\n",
    "with open('../gt_jsons/test_thunen_area_4.json', 'w') as file:\n",
    "    json.dump(test_data, file, ensure_ascii=False)\n",
    "\n",
    "\n",
    "\n",
    "#with open('kfold.json', 'w') as f:\n",
    "#    json.dump(result_list, f)\n",
    "        \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
